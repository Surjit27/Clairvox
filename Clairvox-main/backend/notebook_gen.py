import nbformat as nbf
import os
from typing import List, Tuple

NOTEBOOK_DIR = "notebooks"
os.makedirs(NOTEBOOK_DIR, exist_ok=True)

def create_mini_notebook(claim_text: str, evidence_urls: List[str]) -> Tuple[str, bytes]:
    """
    Generates a Jupyter Notebook to reproduce evidence fetching for a claim.
    Returns the file path and the file content as bytes.
    """
    nb = nbf.v4.new_notebook()

    # --- Header and Claim ---
    header_md = f"""
# Clairvox: Reproducible Evidence Notebook

This notebook contains the resources to independently verify a claim generated by the Clairvox Research Assistant.

**Claim:**
> {claim_text}
"""
    nb['cells'].append(nbf.v4.new_markdown_cell(header_md))

    # --- Code to Fetch URLs ---
    urls_str = "[\n" + ",\n".join([f'    "{url}"' for url in evidence_urls]) + "\n]"
    
    code = f"""
import requests
import textwrap

# List of evidence URLs provided by Clairvox
evidence_urls = {urls_str}

# Standard headers to mimic a browser
headers = {{
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}}

def fetch_and_preview(url):
    \"\"\"Fetches a URL and prints its status and a preview of the title.\"\"\"
    try:
        response = requests.get(url, headers=headers, timeout=10)
        print(f"URL: {{url}}")
        print(f"Status: {{'✅ OK' if response.status_code == 200 else '❌ FAILED'}} (Code: {{response.status_code}})")
        
        # Simple title extraction without heavy parsing
        if response.text and '<title>' in response.text:
            title = response.text.split('<title>')[1].split('</title>')[0]
            print(f"Title: {{textwrap.shorten(title.strip(), width=100)}}")
        print("-" * 30)

    except requests.exceptions.RequestException as e:
        print(f"URL: {{url}}")
        print(f"Status: ❌ FAILED (Error: {{e}})")
        print("-" * 30)


# Iterate and fetch each URL
for url in evidence_urls:
    fetch_and_preview(url)
"""
    nb['cells'].append(nbf.v4.new_code_cell(code))

    # --- Conclusion ---
    conclusion_md = """
## Next Steps
You can now manually inspect the content of these URLs or extend this notebook to perform automated content analysis.
"""
    nb['cells'].append(nbf.v4.new_markdown_cell(conclusion_md))

    # --- Save and Return ---
    # Sanitize claim text for filename
    filename_claim = "".join([c for c in claim_text if c.isalnum() or c in (' ', '-')]).rstrip()
    filename_claim = filename_claim.replace(' ', '_')[:50] # Limit length
    
    file_path = os.path.join(NOTEBOOK_DIR, f"clairvox_evidence_{filename_claim}.ipynb")
    
    notebook_content = nbf.writes(nb)
    
    # In-memory approach for Streamlit's download button
    notebook_bytes = notebook_content.encode('utf-8')

    # Also write to disk for debugging/auditing
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(notebook_content)

    return file_path, notebook_bytes
